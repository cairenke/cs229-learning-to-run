{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Derived from keras-rl\n",
    "import opensim as osim\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.agents import NAFAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "from osim.env import *\n",
    "from osim.http.client import Client\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import argparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command line parameters\n",
    "#parser = argparse.ArgumentParser(description='Train or test neural net motor controller')\n",
    "#parser.add_argument('--train', dest='train', action='store_true', default=True)\n",
    "#parser.add_argument('--test', dest='train', action='store_false', default=True)\n",
    "#parser.add_argument('--steps', dest='steps', action='store', default=10000, type=int)\n",
    "#parser.add_argument('--visualize', dest='visualize', action='store_true', default=False)\n",
    "#parser.add_argument('--model', dest='model', action='store', default=\"example.h5f\")\n",
    "#parser.add_argument('--token', dest='token', action='store', required=False)\n",
    "#args = parser.parse_args()\n",
    "visualize = False\n",
    "steps = 500000\n",
    "train = True\n",
    "model = \"sample1129_CDQN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 41)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1344      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,945\n",
      "Trainable params: 2,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 41)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1344      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 18L)               306       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 18L)               0         \n",
      "=================================================================\n",
      "Total params: 3,234\n",
      "Trainable params: 3,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caire\\Anaconda2\\envs\\opensim-rl\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Elemwise{i..., inputs=[/action_i...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "observation_input (InputLayer)   (None, 1, 41L)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "action_input (InputLayer)        (None, 18L)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 41)            0           observation_input[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 59)            0           action_input[0][0]               \n",
      "                                                                   flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 64)            3840        concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 64)            0           dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 64)            4160        activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 64)            0           dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 171L)          11115       activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 171L)          0           dense_11[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 19,115\n",
      "Trainable params: 19,115\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Training for 500000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 1001/10000 [==>...........................] - ETA: 475s - reward: -0.0063"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 590s - reward: -0.0066   \n",
      "83 episodes - episode_reward: -0.801 [-0.821, -0.786] - loss: 0.001 - mean_absolute_error: 0.024 - mean_q: 1.419\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 589s - reward: -0.0067   \n",
      "83 episodes - episode_reward: -0.802 [-0.823, -0.786] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.197\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 586s - reward: -0.0066   \n",
      "82 episodes - episode_reward: -0.803 [-0.842, -0.786] - loss: 0.000 - mean_absolute_error: 0.004 - mean_q: -0.324\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 585s - reward: -0.0066   \n",
      "82 episodes - episode_reward: -0.806 [-0.824, -0.786] - loss: 0.000 - mean_absolute_error: 0.004 - mean_q: -0.301\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 602s - reward: -0.0066   \n",
      "82 episodes - episode_reward: -0.807 [-0.828, -0.788] - loss: 0.000 - mean_absolute_error: 0.003 - mean_q: -0.353\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 596s - reward: -0.0066   \n",
      "81 episodes - episode_reward: -0.804 [-0.829, -0.785] - loss: 0.000 - mean_absolute_error: 0.002 - mean_q: -0.360\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 596s - reward: -0.0065   \n",
      "81 episodes - episode_reward: -0.807 [-0.829, -0.787] - loss: 0.000 - mean_absolute_error: 0.002 - mean_q: -0.371\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 611s - reward: -0.0065   \n",
      "80 episodes - episode_reward: -0.809 [-0.829, -0.788] - loss: 0.000 - mean_absolute_error: 0.002 - mean_q: -0.392\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 614s - reward: -0.0065   \n",
      "79 episodes - episode_reward: -0.815 [-0.841, -0.799] - loss: 0.000 - mean_absolute_error: 0.002 - mean_q: -0.363\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 633s - reward: -0.0064   \n",
      "80 episodes - episode_reward: -0.814 [-0.859, -0.792] - loss: 0.000 - mean_absolute_error: 0.002 - mean_q: -0.366\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 637s - reward: -0.0065   \n",
      "79 episodes - episode_reward: -0.818 [-0.839, -0.798] - loss: 0.000 - mean_absolute_error: 0.002 - mean_q: -0.388\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 646s - reward: -0.0065   \n",
      "79 episodes - episode_reward: -0.818 [-0.842, -0.795] - loss: 0.000 - mean_absolute_error: 0.002 - mean_q: -0.386\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 656s - reward: -0.0064   \n",
      "78 episodes - episode_reward: -0.817 [-0.870, -0.798] - loss: 0.000 - mean_absolute_error: 0.002 - mean_q: -0.389\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 644s - reward: -0.0064   \n",
      "78 episodes - episode_reward: -0.819 [-0.839, -0.800] - loss: 0.000 - mean_absolute_error: 0.003 - mean_q: -0.329\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 667s - reward: -0.0062    ETA: 0s\n",
      "75 episodes - episode_reward: -0.825 [-0.868, -0.640] - loss: 0.000 - mean_absolute_error: 0.002 - mean_q: -0.366\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 698s - reward: -0.0058   \n",
      "68 episodes - episode_reward: -0.849 [-0.893, -0.816] - loss: 0.000 - mean_absolute_error: 0.002 - mean_q: -0.397\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 777s - reward: -0.0041   \n",
      "59 episodes - episode_reward: -0.696 [-0.947, 0.797] - loss: 0.000 - mean_absolute_error: 0.002 - mean_q: -0.370\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 868s - reward: 1.9394e-04   \n",
      "50 episodes - episode_reward: 0.044 [-0.995, 0.910] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.299\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 841s - reward: -2.6042e-04   \n",
      "48 episodes - episode_reward: -0.052 [-0.950, 0.785] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.251\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 849s - reward: 0.0012   \n",
      "58 episodes - episode_reward: 0.201 [-0.948, 0.785] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: -0.071\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 873s - reward: 0.0016   \n",
      "64 episodes - episode_reward: 0.253 [-0.905, 0.805] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.022\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 904s - reward: 0.0047   \n",
      "71 episodes - episode_reward: 0.656 [-0.898, 0.840] - loss: 0.001 - mean_absolute_error: 0.012 - mean_q: 0.201\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 969s - reward: 0.0034   \n",
      "71 episodes - episode_reward: 0.481 [-0.904, 0.753] - loss: 0.003 - mean_absolute_error: 0.022 - mean_q: 0.796\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 951s - reward: 0.0054   - ETA: 1\n",
      "78 episodes - episode_reward: 0.692 [0.365, 0.802] - loss: 0.004 - mean_absolute_error: 0.024 - mean_q: 1.192\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 1040s - reward: 0.0056  \n",
      "80 episodes - episode_reward: 0.703 [0.272, 0.833] - loss: 0.003 - mean_absolute_error: 0.022 - mean_q: 0.974\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 1301s - reward: 0.0047  - ETA: 0s - reward:\n",
      "85 episodes - episode_reward: 0.551 [-0.785, 0.811] - loss: 0.003 - mean_absolute_error: 0.023 - mean_q: 1.284\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 1212s - reward: 0.0052  \n",
      "84 episodes - episode_reward: 0.613 [-0.900, 0.841] - loss: 0.004 - mean_absolute_error: 0.025 - mean_q: 1.280\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 1212s - reward: 0.0046  \n",
      "84 episodes - episode_reward: 0.549 [-0.923, 0.825] - loss: 0.002 - mean_absolute_error: 0.022 - mean_q: 1.169\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 1102s - reward: 0.0046  \n",
      "77 episodes - episode_reward: 0.601 [-0.984, 0.819] - loss: 0.002 - mean_absolute_error: 0.019 - mean_q: 0.987\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 1068s - reward: 0.0051  \n",
      "82 episodes - episode_reward: 0.610 [-0.964, 0.795] - loss: 0.001 - mean_absolute_error: 0.016 - mean_q: 0.798\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 1201s - reward: 0.0057  \n",
      "103 episodes - episode_reward: 0.553 [-0.850, 0.991] - loss: 0.002 - mean_absolute_error: 0.022 - mean_q: 0.941\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 1208s - reward: 0.0054  \n",
      "93 episodes - episode_reward: 0.583 [0.293, 1.256] - loss: 0.003 - mean_absolute_error: 0.024 - mean_q: 0.989\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 867s - reward: 0.0048   \n",
      "71 episodes - episode_reward: 0.680 [-0.860, 0.878] - loss: 0.007 - mean_absolute_error: 0.039 - mean_q: 1.438\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 1166s - reward: 0.0053  \n",
      "82 episodes - episode_reward: 0.639 [0.172, 0.912] - loss: 0.004 - mean_absolute_error: 0.027 - mean_q: 1.153\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 1033s - reward: 0.0062  \n",
      "89 episodes - episode_reward: 0.698 [0.395, 0.806] - loss: 0.004 - mean_absolute_error: 0.028 - mean_q: 1.192\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 1024s - reward: 0.0062  \n",
      "91 episodes - episode_reward: 0.676 [0.347, 0.777] - loss: 0.002 - mean_absolute_error: 0.019 - mean_q: 0.939\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 1016s - reward: 0.0063  \n",
      "93 episodes - episode_reward: 0.681 [0.458, 0.755] - loss: 0.002 - mean_absolute_error: 0.018 - mean_q: 0.936\n",
      "\n",
      "Interval 38 (370000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 978s - reward: 0.0062   \n",
      "92 episodes - episode_reward: 0.674 [0.447, 0.730] - loss: 0.005 - mean_absolute_error: 0.025 - mean_q: 1.192\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 987s - reward: 0.0064   \n",
      "96 episodes - episode_reward: 0.662 [0.456, 0.887] - loss: 0.012 - mean_absolute_error: 0.046 - mean_q: 2.046\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 965s - reward: 0.0060   \n",
      "88 episodes - episode_reward: 0.687 [0.412, 1.027] - loss: 0.008 - mean_absolute_error: 0.038 - mean_q: 1.678\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 1023s - reward: 0.0059  \n",
      "91 episodes - episode_reward: 0.650 [0.337, 1.051] - loss: 0.005 - mean_absolute_error: 0.030 - mean_q: 1.356\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 946s - reward: 0.0058   \n",
      "82 episodes - episode_reward: 0.707 [0.423, 0.825] - loss: 0.006 - mean_absolute_error: 0.034 - mean_q: 1.076\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 962s - reward: 0.0060   \n",
      "83 episodes - episode_reward: 0.719 [0.633, 0.773] - loss: 0.004 - mean_absolute_error: 0.026 - mean_q: 0.880\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 917s - reward: 0.0055   \n",
      "79 episodes - episode_reward: 0.696 [-0.892, 0.769] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.524\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 932s - reward: 0.0061   \n",
      "87 episodes - episode_reward: 0.698 [0.474, 0.774] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: 0.447\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 959s - reward: 0.0064   \n",
      "90 episodes - episode_reward: 0.706 [0.328, 0.801] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: 0.458\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 957s - reward: 0.0061   \n",
      "87 episodes - episode_reward: 0.696 [0.451, 0.826] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: 0.393\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 975s - reward: 0.0053   \n",
      "74 episodes - episode_reward: 0.718 [-0.868, 0.917] - loss: 0.001 - mean_absolute_error: 0.005 - mean_q: 0.400\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 975s - reward: 0.0057   \n",
      "80 episodes - episode_reward: 0.713 [0.424, 0.839] - loss: 0.001 - mean_absolute_error: 0.006 - mean_q: 0.408\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 994s - reward: 0.0049   \n",
      "done, took 44028.698 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load walking environment\n",
    "env = RunEnv(visualize)\n",
    "env.reset()\n",
    "\n",
    "nb_actions = env.action_space.shape[0]\n",
    "\n",
    "# Total number of steps in training\n",
    "nallsteps = steps\n",
    "\n",
    "# Create networks for DDPG\n",
    "# Build all necessary models: V, mu, and L networks.\n",
    "V_model = Sequential()\n",
    "V_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "V_model.add(Dense(32))\n",
    "V_model.add(Activation('relu'))\n",
    "V_model.add(Dense(32))\n",
    "V_model.add(Activation('relu'))\n",
    "V_model.add(Dense(16))\n",
    "V_model.add(Activation('relu'))\n",
    "V_model.add(Dense(1))\n",
    "V_model.add(Activation('linear'))\n",
    "print(V_model.summary())\n",
    "\n",
    "mu_model = Sequential()\n",
    "mu_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "mu_model.add(Dense(32))\n",
    "mu_model.add(Activation('relu'))\n",
    "mu_model.add(Dense(32))\n",
    "mu_model.add(Activation('relu'))\n",
    "mu_model.add(Dense(16))\n",
    "mu_model.add(Activation('relu'))\n",
    "mu_model.add(Dense(nb_actions))\n",
    "mu_model.add(Activation('sigmoid'))\n",
    "print(mu_model.summary())\n",
    "\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "x = concatenate([action_input, Flatten()(observation_input)])\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(((nb_actions * nb_actions + nb_actions) // 2))(x)\n",
    "x = Activation('linear')(x)\n",
    "L_model = Model(input=[action_input, observation_input], output=x)\n",
    "print(L_model.summary())\n",
    "\n",
    "# Set up the agent for training\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(theta=.15, mu=0., sigma=.2, size=env.noutput)\n",
    "#agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n",
    "#                 memory=memory, nb_steps_warmup_critic=100, nb_steps_warmup_actor=100,\n",
    "#                  random_process=random_process, gamma=.99, target_model_update=1e-3,\n",
    "#                  delta_clip=1.)\n",
    "agent = NAFAgent(nb_actions=env.noutput, V_model=V_model, L_model=L_model, mu_model=mu_model,\n",
    "                           memory=memory, nb_steps_warmup=1000, random_process=random_process,\n",
    "                           gamma=.99, target_model_update=0.1)\n",
    "agent.compile(Adam(lr=.001, clipnorm=1.), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "if train:\n",
    "    agent.fit(env, nb_steps=nallsteps, visualize=False, verbose=1, nb_max_episode_steps=env.timestep_limit, log_interval=10000)\n",
    "    # After training is done, we save the final weights.\n",
    "    agent.save_weights(model, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 0.421, steps: 74\n"
     ]
    }
   ],
   "source": [
    "# If TEST and no TOKEN, run some test experiments\n",
    "train = False\n",
    "env = RunEnv(visualize=True)\n",
    "if not train:\n",
    "    agent.load_weights(model)\n",
    "    # Finally, evaluate our algorithm for 1 episode.\n",
    "    agent.test(env, nb_episodes=1, visualize=True, nb_max_episode_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
