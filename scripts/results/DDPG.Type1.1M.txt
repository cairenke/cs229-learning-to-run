=============================================================================
       __|  __|_  )
       _|  (     /   Deep Learning AMI (Amazon Linux)
      ___|\___|___|
=============================================================================

Please use one of the following commands to start the required environment with the framework of your choice:
for MXNet(+Keras1) with Python3 (CUDA 9) _____________________ source activate mxnet_p36
for MXNet(+Keras1) with Python2 (CUDA 9) _____________________ source activate mxnet_p27
for TensorFlow(+Keras2) with Python3 (CUDA 8) ________________ source activate tensorflow_p36
for TensorFlow(+Keras2) with Python2 (CUDA 8) ________________ source activate tensorflow_p27
for Theano(+Keras2) with Python3 (CUDA 8) ____________________ source activate theano_p36
for Theano(+Keras2) with Python2 (CUDA 8) ____________________ source activate theano_p27
for PyTorch with Python3 (CUDA 8) ____________________________ source activate pytorch_p36
for PyTorch with Python2 (CUDA 8) ____________________________ source activate pytorch_p27
for CNTK(+Keras2) with Python3 (CUDA 8) ______________________ source activate cntk_p36
for CNTK(+Keras2) with Python2 (CUDA 8) ______________________ source activate cntk_p27
for Caffe2 with Python2 (CUDA 9) _____________________________ source activate caffe2_p27
for base Python2 (CUDA 9) ____________________________________ source activate python2
for base Python3 (CUDA 9) ____________________________________ source activate python3

Official conda user guide: https://conda.io/docs/user-guide/index.html
AMI details: https://aws.amazon.com/amazon-ai/amis/details/
Release Notes: https://aws.amazon.com/documentation/dlami/latest/devguide/appendix-ami-release-notes.html

[ec2-user@ip-172-31-15-93 ~]$ screen -r

Interval 82 (810000 steps performed)
10000/10000 [==============================] - 2738s - reward: 0.0108
31 episodes - episode_reward: 3.617 [0.071, 10.123] - loss: 0.002 - mean_absolute_error: 0.026 - mean_q: 2.952

Interval 83 (820000 steps performed)
10000/10000 [==============================] - 1821s - reward: 0.0107
38 episodes - episode_reward: 2.750 [0.073, 8.221] - loss: 0.002 - mean_absolute_error: 0.027 - mean_q: 2.954

Interval 84 (830000 steps performed)
10000/10000 [==============================] - 1135s - reward: 0.0092
25 episodes - episode_reward: 3.786 [1.287, 8.279] - loss: 0.002 - mean_absolute_error: 0.028 - mean_q: 2.993

Interval 85 (840000 steps performed)
10000/10000 [==============================] - 595s - reward: 0.0078
13 episodes - episode_reward: 5.525 [0.858, 9.221] - loss: 0.002 - mean_absolute_error: 0.029 - mean_q: 3.039

Interval 86 (850000 steps performed)
10000/10000 [==============================] - 1020s - reward: 0.0092
18 episodes - episode_reward: 5.404 [0.783, 9.884] - loss: 0.002 - mean_absolute_error: 0.029 - mean_q: 3.030

Interval 87 (860000 steps performed)
10000/10000 [==============================] - 975s - reward: 0.0092
27 episodes - episode_reward: 3.308 [0.378, 10.482] - loss: 0.002 - mean_absolute_error: 0.030 - mean_q: 2.979

Interval 88 (870000 steps performed)
10000/10000 [==============================] - 670s - reward: 0.0098
17 episodes - episode_reward: 5.759 [1.256, 10.010] - loss: 0.002 - mean_absolute_error: 0.029 - mean_q: 2.976

Interval 89 (880000 steps performed)
10000/10000 [==============================] - 667s - reward: 0.0086
19 episodes - episode_reward: 4.601 [0.645, 9.826] - loss: 0.002 - mean_absolute_error: 0.030 - mean_q: 2.966

Interval 90 (890000 steps performed)
10000/10000 [==============================] - 1061s - reward: 0.0087
17 episodes - episode_reward: 4.989 [0.848, 9.837] - loss: 0.002 - mean_absolute_error: 0.028 - mean_q: 2.935

Interval 91 (900000 steps performed)
10000/10000 [==============================] - 2139s - reward: 0.0103
20 episodes - episode_reward: 5.390 [1.093, 9.918] - loss: 0.002 - mean_absolute_error: 0.027 - mean_q: 2.920

Interval 92 (910000 steps performed)
10000/10000 [==============================] - 2223s - reward: 0.0108
21 episodes - episode_reward: 5.069 [2.531, 9.942] - loss: 0.002 - mean_absolute_error: 0.025 - mean_q: 2.904

Interval 93 (920000 steps performed)
10000/10000 [==============================] - 2353s - reward: 0.0120
38 episodes - episode_reward: 3.200 [1.430, 7.770] - loss: 0.002 - mean_absolute_error: 0.023 - mean_q: 2.933

Interval 94 (930000 steps performed)
10000/10000 [==============================] - 2625s - reward: 0.0103
21 episodes - episode_reward: 4.945 [0.962, 9.943] - loss: 0.002 - mean_absolute_error: 0.022 - mean_q: 2.933

Interval 95 (940000 steps performed)
10000/10000 [==============================] - 1762s - reward: 0.0102
16 episodes - episode_reward: 6.373 [1.713, 9.918] - loss: 0.002 - mean_absolute_error: 0.021 - mean_q: 2.936

Interval 96 (950000 steps performed)
10000/10000 [==============================] - 1465s - reward: 0.0097
13 episodes - episode_reward: 7.411 [3.562, 10.730] - loss: 0.002 - mean_absolute_error: 0.021 - mean_q: 2.907

Interval 97 (960000 steps performed)
10000/10000 [==============================] - 830s - reward: 0.0082
26 episodes - episode_reward: 3.167 [0.131, 9.777] - loss: 0.001 - mean_absolute_error: 0.019 - mean_q: 2.870

Interval 98 (970000 steps performed)
10000/10000 [==============================] - 1840s - reward: 0.0088
16 episodes - episode_reward: 5.430 [1.272, 9.831] - loss: 0.001 - mean_absolute_error: 0.019 - mean_q: 2.803

Interval 99 (980000 steps performed)
10000/10000 [==============================] - 2097s - reward: 0.0096
14 episodes - episode_reward: 6.780 [1.781, 9.902] - loss: 0.001 - mean_absolute_error: 0.020 - mean_q: 2.789

Interval 100 (990000 steps performed)
10000/10000 [==============================] - 1742s - reward: 0.0089
done, took 145606.131 seconds
