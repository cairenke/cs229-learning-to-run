[ec2-user@ip-172-31-32-61 ~]$ screen -r

10000/10000 [==============================] - 1313s - reward: 0.0036
46 episodes - episode_reward: 0.785 [-0.890, 1.343] - loss: 0.001 - mean_absolute_error: 0.021 - mean_q: 1.267

Interval 28 (270000 steps performed)
10000/10000 [==============================] - 1067s - reward: 0.0014
28 episodes - episode_reward: 0.532 [-0.832, 1.404] - loss: 0.001 - mean_absolute_error: 0.021 - mean_q: 1.252

Interval 29 (280000 steps performed)
10000/10000 [==============================] - 1559s - reward: 0.0016
26 episodes - episode_reward: 0.612 [-0.875, 1.973] - loss: 0.001 - mean_absolute_error: 0.020 - mean_q: 1.279

Interval 30 (290000 steps performed)
10000/10000 [==============================] - 1386s - reward: 6.8556e-04
23 episodes - episode_reward: 0.271 [-0.882, 1.395] - loss: 0.001 - mean_absolute_error: 0.019 - mean_q: 1.296

Interval 31 (300000 steps performed)
10000/10000 [==============================] - 1655s - reward: 0.0024
31 episodes - episode_reward: 0.778 [-0.857, 1.586] - loss: 0.001 - mean_absolute_error: 0.018 - mean_q: 1.312

Interval 32 (310000 steps performed)
10000/10000 [==============================] - 1900s - reward: 0.0033
37 episodes - episode_reward: 0.895 [-0.620, 1.662] - loss: 0.001 - mean_absolute_error: 0.018 - mean_q: 1.325

Interval 33 (320000 steps performed)
10000/10000 [==============================] - 1381s - reward: 0.0033
34 episodes - episode_reward: 0.976 [-0.623, 1.718] - loss: 0.001 - mean_absolute_error: 0.018 - mean_q: 1.318

Interval 34 (330000 steps performed)
10000/10000 [==============================] - 948s - reward: 0.0027
36 episodes - episode_reward: 0.722 [-0.967, 1.294] - loss: 0.001 - mean_absolute_error: 0.018 - mean_q: 1.315

Interval 35 (340000 steps performed)
10000/10000 [==============================] - 1215s - reward: 0.0028
45 episodes - episode_reward: 0.649 [-1.341, 1.405] - loss: 0.001 - mean_absolute_error: 0.018 - mean_q: 1.286

Interval 36 (350000 steps performed)
10000/10000 [==============================] - 902s - reward: -4.6579e-04
30 episodes - episode_reward: -0.176 [-1.078, 1.360] - loss: 0.001 - mean_absolute_error: 0.018 - mean_q: 1.266

Interval 37 (360000 steps performed)
10000/10000 [==============================] - 1352s - reward: 0.0020
44 episodes - episode_reward: 0.474 [-1.195, 1.549] - loss: 0.001 - mean_absolute_error: 0.019 - mean_q: 1.248

Interval 38 (370000 steps performed)
10000/10000 [==============================] - 1585s - reward: 0.0042
52 episodes - episode_reward: 0.812 [-0.029, 1.611] - loss: 0.001 - mean_absolute_error: 0.019 - mean_q: 1.228

Interval 39 (380000 steps performed)
10000/10000 [==============================] - 1566s - reward: 0.0032
34 episodes - episode_reward: 0.910 [-0.438, 1.712] - loss: 0.001 - mean_absolute_error: 0.018 - mean_q: 1.193

Interval 40 (390000 steps performed)
10000/10000 [==============================] - 1231s - reward: 0.0019
25 episodes - episode_reward: 0.785 [-0.836, 1.633] - loss: 0.001 - mean_absolute_error: 0.018 - mean_q: 1.170

Interval 41 (400000 steps performed)
10000/10000 [==============================] - 1457s - reward: 0.0021
41 episodes - episode_reward: 0.500 [-1.060, 1.390] - loss: 0.000 - mean_absolute_error: 0.017 - mean_q: 1.142

Interval 42 (410000 steps performed)
10000/10000 [==============================] - 1307s - reward: 0.0013
33 episodes - episode_reward: 0.397 [-0.868, 1.672] - loss: 0.000 - mean_absolute_error: 0.016 - mean_q: 1.100

Interval 43 (420000 steps performed)
10000/10000 [==============================] - 1289s - reward: 0.0024
23 episodes - episode_reward: 1.013 [0.170, 2.053] - loss: 0.000 - mean_absolute_error: 0.015 - mean_q: 1.075

Interval 44 (430000 steps performed)
10000/10000 [==============================] - 1556s - reward: 0.0021
44 episodes - episode_reward: 0.481 [-0.850, 1.507] - loss: 0.000 - mean_absolute_error: 0.015 - mean_q: 1.057

Interval 45 (440000 steps performed)
10000/10000 [==============================] - 1517s - reward: 0.0021
24 episodes - episode_reward: 0.848 [-0.757, 1.779] - loss: 0.001 - mean_absolute_error: 0.015 - mean_q: 1.042

Interval 46 (450000 steps performed)
10000/10000 [==============================] - 1509s - reward: 0.0018
38 episodes - episode_reward: 0.509 [-0.807, 1.794] - loss: 0.000 - mean_absolute_error: 0.015 - mean_q: 1.030

Interval 47 (460000 steps performed)
10000/10000 [==============================] - 1236s - reward: 0.0012
19 episodes - episode_reward: 0.623 [-0.776, 1.630] - loss: 0.000 - mean_absolute_error: 0.014 - mean_q: 1.021

Interval 48 (470000 steps performed)
10000/10000 [==============================] - 1519s - reward: -0.0013
28 episodes - episode_reward: -0.466 [-1.116, 1.072] - loss: 0.000 - mean_absolute_error: 0.014 - mean_q: 1.010

Interval 49 (480000 steps performed)
10000/10000 [==============================] - 1003s - reward: 9.0256e-05
27 episodes - episode_reward: 0.036 [-0.868, 0.991] - loss: 0.000 - mean_absolute_error: 0.014 - mean_q: 0.989

Interval 50 (490000 steps performed)
10000/10000 [==============================] - 1105s - reward: 0.0020
done, took 70951.310 seconds