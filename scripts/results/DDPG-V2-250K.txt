Training for 250000 steps ...
Interval 1 (0 steps performed)
10000/10000 [==============================] - 824s - reward: 2.6940e-04
83 episodes - episode_reward: 0.029 [-0.045, 0.255] - loss: 0.001 - mean_absolute_error: 0.023 - mean_q: -0.347

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 617s - reward: 0.0043
68 episodes - episode_reward: 0.633 [-0.514, 2.951] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: -0.200

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 483s - reward: 0.0133
79 episodes - episode_reward: 1.679 [0.530, 3.072] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.012

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 489s - reward: 0.0104
74 episodes - episode_reward: 1.402 [0.057, 2.219] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: 0.135

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 2335s - reward: 0.0070
103 episodes - episode_reward: 0.680 [0.108, 4.521] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: 0.221

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 1289s - reward: 0.0099
75 episodes - episode_reward: 1.303 [0.096, 3.566] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: 0.295

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 1012s - reward: 0.0166
82 episodes - episode_reward: 2.041 [1.761, 3.438] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.406

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 557s - reward: 0.0169
83 episodes - episode_reward: 2.027 [1.848, 2.414] - loss: 0.000 - mean_absolute_error: 0.011 - mean_q: 0.540

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 565s - reward: 0.0165
86 episodes - episode_reward: 1.933 [0.416, 2.366] - loss: 0.000 - mean_absolute_error: 0.013 - mean_q: 0.676

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 672s - reward: 0.0156
85 episodes - episode_reward: 1.837 [0.278, 3.760] - loss: 0.001 - mean_absolute_error: 0.015 - mean_q: 0.792

Interval 11 (100000 steps performed)
10000/10000 [==============================] - 2287s - reward: 0.0136
92 episodes - episode_reward: 1.476 [0.008, 3.039] - loss: 0.001 - mean_absolute_error: 0.017 - mean_q: 0.939

Interval 12 (110000 steps performed)
10000/10000 [==============================] - 2578s - reward: 0.0129
63 episodes - episode_reward: 2.043 [0.680, 3.702] - loss: 0.001 - mean_absolute_error: 0.019 - mean_q: 1.130

Interval 13 (120000 steps performed)
10000/10000 [==============================] - 2309s - reward: 0.0132
65 episodes - episode_reward: 2.051 [0.432, 6.038] - loss: 0.001 - mean_absolute_error: 0.019 - mean_q: 1.251

Interval 14 (130000 steps performed)
10000/10000 [==============================] - 1265s - reward: 0.0124
53 episodes - episode_reward: 2.293 [0.718, 8.905] - loss: 0.001 - mean_absolute_error: 0.019 - mean_q: 1.313

Interval 15 (140000 steps performed)
10000/10000 [==============================] - 730s - reward: 0.0147
64 episodes - episode_reward: 2.304 [0.471, 3.482] - loss: 0.001 - mean_absolute_error: 0.018 - mean_q: 1.363

Interval 16 (150000 steps performed)
10000/10000 [==============================] - 711s - reward: 0.0151
62 episodes - episode_reward: 2.437 [1.800, 4.073] - loss: 0.000 - mean_absolute_error: 0.016 - mean_q: 1.413

Interval 17 (160000 steps performed)
10000/10000 [==============================] - 709s - reward: 0.0137
55 episodes - episode_reward: 2.494 [1.396, 6.809] - loss: 0.000 - mean_absolute_error: 0.016 - mean_q: 1.461

Interval 18 (170000 steps performed)
10000/10000 [==============================] - 840s - reward: 0.0122
63 episodes - episode_reward: 1.969 [0.411, 3.659] - loss: 0.000 - mean_absolute_error: 0.017 - mean_q: 1.506

Interval 19 (180000 steps performed)
10000/10000 [==============================] - 871s - reward: 0.0124
60 episodes - episode_reward: 2.055 [0.369, 4.362] - loss: 0.001 - mean_absolute_error: 0.018 - mean_q: 1.554

Interval 20 (190000 steps performed)
10000/10000 [==============================] - 724s - reward: 0.0105
54 episodes - episode_reward: 1.947 [0.165, 4.496] - loss: 0.001 - mean_absolute_error: 0.018 - mean_q: 1.595

Interval 21 (200000 steps performed)
10000/10000 [==============================] - 626s - reward: 0.0102
47 episodes - episode_reward: 2.115 [0.113, 4.310] - loss: 0.001 - mean_absolute_error: 0.018 - mean_q: 1.604

Interval 22 (210000 steps performed)
10000/10000 [==============================] - 745s - reward: 0.0109
41 episodes - episode_reward: 2.711 [0.381, 6.335] - loss: 0.001 - mean_absolute_error: 0.018 - mean_q: 1.600

Interval 23 (220000 steps performed)
10000/10000 [==============================] - 1411s - reward: 0.0109
39 episodes - episode_reward: 2.846 [0.406, 10.491] - loss: 0.001 - mean_absolute_error: 0.018 - mean_q: 1.611

Interval 24 (230000 steps performed)
10000/10000 [==============================] - 1824s - reward: 0.0106
60 episodes - episode_reward: 1.734 [0.158, 7.068] - loss: 0.001 - mean_absolute_error: 0.019 - mean_q: 1.623

Interval 25 (240000 steps performed)
10000/10000 [==============================] - 1545s - reward: 0.0116
done, took 28031.576 seconds
