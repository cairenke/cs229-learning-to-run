54 episodes - episode_reward: 0.809 [-0.727, 1.923] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.454

Interval 28 (270000 steps performed)
10000/10000 [==============================] - 1889s - reward: 0.0077
70 episodes - episode_reward: 1.094 [0.126, 2.003] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.472

Interval 29 (280000 steps performed)
10000/10000 [==============================] - 1405s - reward: 0.0063
48 episodes - episode_reward: 1.319 [0.204, 1.960] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.499

Interval 30 (290000 steps performed)
10000/10000 [==============================] - 1846s - reward: 0.0060
69 episodes - episode_reward: 0.867 [-0.178, 1.885] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.523

Interval 31 (300000 steps performed)
10000/10000 [==============================] - 1937s - reward: 0.0079
74 episodes - episode_reward: 1.053 [0.137, 2.003] - loss: 0.000 - mean_absolute_error: 0.011 - mean_q: 0.538

Interval 32 (310000 steps performed)
10000/10000 [==============================] - 2303s - reward: 0.0079
74 episodes - episode_reward: 1.082 [0.156, 1.881] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.553

Interval 33 (320000 steps performed)
10000/10000 [==============================] - 2238s - reward: 0.0069
73 episodes - episode_reward: 0.953 [0.273, 2.106] - loss: 0.000 - mean_absolute_error: 0.011 - mean_q: 0.560

Interval 34 (330000 steps performed)
10000/10000 [==============================] - 1540s - reward: 0.0063
49 episodes - episode_reward: 1.266 [0.188, 2.151] - loss: 0.000 - mean_absolute_error: 0.011 - mean_q: 0.575

Interval 35 (340000 steps performed)
10000/10000 [==============================] - 1495s - reward: 0.0083
65 episodes - episode_reward: 1.289 [0.175, 2.055] - loss: 0.000 - mean_absolute_error: 0.011 - mean_q: 0.593

Interval 36 (350000 steps performed)
10000/10000 [==============================] - 1780s - reward: 0.0087
59 episodes - episode_reward: 1.473 [0.137, 2.219] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.610

Interval 37 (360000 steps performed)
10000/10000 [==============================] - 1631s - reward: 0.0086
55 episodes - episode_reward: 1.549 [0.225, 2.343] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.631

Interval 38 (370000 steps performed)
10000/10000 [==============================] - 1863s - reward: 0.0099
55 episodes - episode_reward: 1.800 [0.272, 2.574] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.648

Interval 39 (380000 steps performed)
10000/10000 [==============================] - 2298s - reward: 0.0086
66 episodes - episode_reward: 1.298 [-0.106, 2.412] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.661

Interval 40 (390000 steps performed)
10000/10000 [==============================] - 2310s - reward: 0.0073
50 episodes - episode_reward: 1.463 [0.637, 2.630] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.670

Interval 41 (400000 steps performed)
10000/10000 [==============================] - 1565s - reward: 0.0076
57 episodes - episode_reward: 1.328 [0.254, 2.593] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.678

Interval 42 (410000 steps performed)
10000/10000 [==============================] - 1985s - reward: 0.0057
45 episodes - episode_reward: 1.263 [0.136, 2.538] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.678

Interval 43 (420000 steps performed)
10000/10000 [==============================] - 1896s - reward: 0.0061
46 episodes - episode_reward: 1.343 [0.195, 2.163] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.681

Interval 44 (430000 steps performed)
10000/10000 [==============================] - 1688s - reward: 0.0044
40 episodes - episode_reward: 1.099 [-0.350, 2.483] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.688

Interval 45 (440000 steps performed)
10000/10000 [==============================] - 2149s - reward: 0.0087
48 episodes - episode_reward: 1.820 [0.387, 2.423] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.690

Interval 46 (450000 steps performed)
10000/10000 [==============================] - 1826s - reward: 0.0102
59 episodes - episode_reward: 1.724 [0.474, 2.574] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.698

Interval 47 (460000 steps performed)
10000/10000 [==============================] - 2318s - reward: 0.0106
78 episodes - episode_reward: 1.364 [0.304, 2.161] - loss: 0.000 - mean_absolute_error: 0.011 - mean_q: 0.712

Interval 48 (470000 steps performed)
10000/10000 [==============================] - 2053s - reward: 0.0104
70 episodes - episode_reward: 1.489 [0.308, 2.238] - loss: 0.000 - mean_absolute_error: 0.011 - mean_q: 0.728

Interval 49 (480000 steps performed)
10000/10000 [==============================] - 2135s - reward: 0.0090
50 episodes - episode_reward: 1.796 [0.951, 2.371] - loss: 0.000 - mean_absolute_error: 0.011 - mean_q: 0.740

Interval 50 (490000 steps performed)
10000/10000 [==============================] - 1838s - reward: 0.0073
done, took 80530.703 seconds