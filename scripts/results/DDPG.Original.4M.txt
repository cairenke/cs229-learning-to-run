
81 episodes - episode_reward: 0.788 [0.527, 1.224] - loss: 0.000 - mean_absolute_error: 0.004 - mean_q: 0.415

Interval 384 (3830000 steps performed)
10000/10000 [==============================] - 821s - reward: 0.0060
85 episodes - episode_reward: 0.706 [0.509, 1.056] - loss: 0.000 - mean_absolute_error: 0.004 - mean_q: 0.420

Interval 385 (3840000 steps performed)
10000/10000 [==============================] - 1256s - reward: 0.0061
92 episodes - episode_reward: 0.663 [0.512, 1.007] - loss: 0.000 - mean_absolute_error: 0.004 - mean_q: 0.423

Interval 386 (3850000 steps performed)
10000/10000 [==============================] - 1458s - reward: 0.0072
87 episodes - episode_reward: 0.825 [0.616, 1.033] - loss: 0.000 - mean_absolute_error: 0.004 - mean_q: 0.421

Interval 387 (3860000 steps performed)
10000/10000 [==============================] - 1323s - reward: 0.0063
84 episodes - episode_reward: 0.745 [0.516, 1.185] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: 0.420

Interval 388 (3870000 steps performed)
10000/10000 [==============================] - 1025s - reward: 0.0043
54 episodes - episode_reward: 0.809 [-0.535, 1.226] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: 0.419

Interval 389 (3880000 steps performed)
10000/10000 [==============================] - 1062s - reward: 0.0023
46 episodes - episode_reward: 0.486 [-0.914, 1.031] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: 0.423

Interval 390 (3890000 steps performed)
10000/10000 [==============================] - 853s - reward: 0.0013
32 episodes - episode_reward: 0.410 [-0.861, 1.011] - loss: 0.000 - mean_absolute_error: 0.004 - mean_q: 0.414

Interval 391 (3900000 steps performed)
10000/10000 [==============================] - 1189s - reward: 4.1432e-04
33 episodes - episode_reward: 0.125 [-0.625, 1.434] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: 0.397

Interval 392 (3910000 steps performed)
10000/10000 [==============================] - 1143s - reward: 5.3406e-04
38 episodes - episode_reward: 0.150 [-0.413, 1.050] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: 0.371

Interval 393 (3920000 steps performed)
10000/10000 [==============================] - 890s - reward: 4.3935e-04
19 episodes - episode_reward: 0.200 [-0.427, 0.812] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: 0.344

Interval 394 (3930000 steps performed)
10000/10000 [==============================] - 1264s - reward: 0.0017
23 episodes - episode_reward: 0.740 [0.355, 1.891] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: 0.326

Interval 395 (3940000 steps performed)
10000/10000 [==============================] - 2758s - reward: 0.0046
57 episodes - episode_reward: 0.823 [-0.266, 1.806] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: 0.312

Interval 396 (3950000 steps performed)
10000/10000 [==============================] - 2638s - reward: 0.0052
53 episodes - episode_reward: 0.971 [-0.183, 1.803] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: 0.309

Interval 397 (3960000 steps performed)
10000/10000 [==============================] - 1459s - reward: 0.0028
22 episodes - episode_reward: 1.255 [-0.399, 1.995] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: 0.299

Interval 398 (3970000 steps performed)
10000/10000 [==============================] - 1511s - reward: 0.0044
34 episodes - episode_reward: 1.283 [-0.369, 1.712] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: 0.296

Interval 399 (3980000 steps performed)
10000/10000 [==============================] - 2587s - reward: 0.0069
62 episodes - episode_reward: 1.120 [-0.542, 1.864] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: 0.303

Interval 400 (3990000 steps performed)
10000/10000 [==============================] - 3533s - reward: 0.0066
done, took 872869.707 seconds