Training for 500000 steps ...
Interval 1 (0 steps performed)
10000/10000 [==============================] - 525s - reward: -0.0067
87 episodes - episode_reward: -0.774 [-0.972, 0.091] - loss: 0.000 - mean_absolute_error: 0.011 - mean_q: 0.042

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 560s - reward: -0.0065
80 episodes - episode_reward: -0.816 [-0.909, -0.619] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: 0.028

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 623s - reward: -0.0067
84 episodes - episode_reward: -0.797 [-1.265, -0.498] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.023

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 843s - reward: -0.0047
61 episodes - episode_reward: -0.782 [-1.658, 0.720] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.026

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 1248s - reward: -0.0030
67 episodes - episode_reward: -0.446 [-0.950, 0.840] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: 0.000

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 902s - reward: 0.0044
70 episodes - episode_reward: 0.622 [-0.828, 1.061] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: 0.018

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 979s - reward: 0.0058
105 episodes - episode_reward: 0.555 [-1.069, 1.046] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: 0.051

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 1604s - reward: 0.0052
122 episodes - episode_reward: 0.422 [-1.076, 1.097] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: 0.081

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 1048s - reward: 0.0062
101 episodes - episode_reward: 0.610 [0.252, 0.863] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: 0.112

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 1732s - reward: 0.0067
97 episodes - episode_reward: 0.690 [0.360, 0.931] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: 0.144

Interval 11 (100000 steps performed)
10000/10000 [==============================] - 2590s - reward: 0.0063
93 episodes - episode_reward: 0.678 [0.279, 1.513] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: 0.199

Interval 12 (110000 steps performed)
10000/10000 [==============================] - 1492s - reward: 0.0051
52 episodes - episode_reward: 0.982 [-0.047, 1.764] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: 0.269

Interval 13 (120000 steps performed)
10000/10000 [==============================] - 1713s - reward: 0.0088
87 episodes - episode_reward: 1.016 [0.072, 1.777] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.345

Interval 14 (130000 steps performed)
10000/10000 [==============================] - 1524s - reward: 0.0080
103 episodes - episode_reward: 0.778 [0.434, 1.420] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.411

Interval 15 (140000 steps performed)
10000/10000 [==============================] - 1493s - reward: 0.0047
85 episodes - episode_reward: 0.558 [-0.911, 1.439] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.452

Interval 16 (150000 steps performed)
10000/10000 [==============================] - 799s - reward: 0.0054
79 episodes - episode_reward: 0.685 [-0.856, 1.207] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.460

Interval 17 (160000 steps performed)
10000/10000 [==============================] - 966s - reward: 0.0065
90 episodes - episode_reward: 0.722 [-0.564, 1.608] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: 0.468

Interval 18 (170000 steps performed)
10000/10000 [==============================] - 962s - reward: 0.0046
67 episodes - episode_reward: 0.687 [-0.859, 1.894] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: 0.478

Interval 19 (180000 steps performed)
10000/10000 [==============================] - 1509s - reward: 0.0041
50 episodes - episode_reward: 0.813 [-1.102, 2.053] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.490

Interval 20 (190000 steps performed)
10000/10000 [==============================] - 1273s - reward: 0.0056
44 episodes - episode_reward: 1.290 [-0.505, 1.826] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.502

Interval 21 (200000 steps performed)
10000/10000 [==============================] - 1407s - reward: 0.0064
50 episodes - episode_reward: 1.287 [0.303, 2.078] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.512

Interval 22 (210000 steps performed)
10000/10000 [==============================] - 1630s - reward: 0.0092
59 episodes - episode_reward: 1.546 [0.567, 1.879] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.528

Interval 23 (220000 steps performed)
10000/10000 [==============================] - 1291s - reward: 0.0094
58 episodes - episode_reward: 1.622 [0.715, 2.413] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.544

Interval 24 (230000 steps performed)
10000/10000 [==============================] - 1223s - reward: 0.0072
52 episodes - episode_reward: 1.387 [0.397, 2.232] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: 0.559

Interval 25 (240000 steps performed)
10000/10000 [==============================] - 1450s - reward: 0.0078
52 episodes - episode_reward: 1.522 [-0.026, 2.708] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: 0.583

Interval 26 (250000 steps performed)
10000/10000 [==============================] - 1453s - reward: 0.0092
64 episodes - episode_reward: 1.434 [0.428, 2.289] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.605

Interval 27 (260000 steps performed)
10000/10000 [==============================] - 1253s - reward: 0.0092
61 episodes - episode_reward: 1.509 [0.315, 2.390] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.624

Interval 28 (270000 steps performed)
10000/10000 [==============================] - 1349s - reward: 0.0118
65 episodes - episode_reward: 1.814 [0.911, 2.588] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.650

Interval 29 (280000 steps performed)
10000/10000 [==============================] - 1296s - reward: 0.0109
58 episodes - episode_reward: 1.844 [-0.934, 2.432] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.680

Interval 30 (290000 steps performed)
10000/10000 [==============================] - 1233s - reward: 0.0100
54 episodes - episode_reward: 1.883 [0.307, 2.881] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.707

Interval 31 (300000 steps performed)
10000/10000 [==============================] - 1193s - reward: 0.0094
50 episodes - episode_reward: 1.868 [0.342, 2.408] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.730

Interval 32 (310000 steps performed)
10000/10000 [==============================] - 1590s - reward: 0.0077
46 episodes - episode_reward: 1.650 [-0.944, 2.428] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: 0.735

Interval 33 (320000 steps performed)
10000/10000 [==============================] - 1247s - reward: 0.0080
47 episodes - episode_reward: 1.741 [0.061, 2.803] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.739

Interval 34 (330000 steps performed)
10000/10000 [==============================] - 1269s - reward: 0.0082
49 episodes - episode_reward: 1.682 [-0.500, 2.423] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.746

Interval 35 (340000 steps performed)
10000/10000 [==============================] - 1635s - reward: 0.0057
45 episodes - episode_reward: 1.258 [0.038, 2.377] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.765

Interval 36 (350000 steps performed)
10000/10000 [==============================] - 1274s - reward: 0.0117
64 episodes - episode_reward: 1.826 [0.378, 3.050] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: 0.783

Interval 37 (360000 steps performed)
10000/10000 [==============================] - 1485s - reward: 0.0125
84 episodes - episode_reward: 1.491 [-0.128, 2.475] - loss: 0.000 - mean_absolute_error: 0.011 - mean_q: 0.814

Interval 38 (370000 steps performed)
10000/10000 [==============================] - 1358s - reward: 0.0097
68 episodes - episode_reward: 1.424 [-0.276, 2.720] - loss: 0.000 - mean_absolute_error: 0.011 - mean_q: 0.833

Interval 39 (380000 steps performed)
10000/10000 [==============================] - 1539s - reward: 0.0085
62 episodes - episode_reward: 1.359 [0.192, 3.019] - loss: 0.000 - mean_absolute_error: 0.012 - mean_q: 0.840

Interval 40 (390000 steps performed)
10000/10000 [==============================] - 1890s - reward: 0.0083
56 episodes - episode_reward: 1.484 [-0.425, 2.783] - loss: 0.000 - mean_absolute_error: 0.013 - mean_q: 0.842

Interval 41 (400000 steps performed)
10000/10000 [==============================] - 1624s - reward: 0.0097
58 episodes - episode_reward: 1.689 [-0.857, 2.553] - loss: 0.000 - mean_absolute_error: 0.013 - mean_q: 0.844

Interval 42 (410000 steps performed)
10000/10000 [==============================] - 2945s - reward: 0.0074
94 episodes - episode_reward: 0.783 [-0.706, 2.709] - loss: 0.000 - mean_absolute_error: 0.014 - mean_q: 0.861

Interval 43 (420000 steps performed)
10000/10000 [==============================] - 1123s - reward: 0.0049
35 episodes - episode_reward: 1.398 [-0.242, 3.920] - loss: 0.000 - mean_absolute_error: 0.014 - mean_q: 0.872

Interval 44 (430000 steps performed)
10000/10000 [==============================] - 3409s - reward: 0.0018
162 episodes - episode_reward: 0.110 [-1.386, 2.340] - loss: 0.001 - mean_absolute_error: 0.016 - mean_q: 0.882

Interval 45 (440000 steps performed)
10000/10000 [==============================] - 1848s - reward: 0.0059
75 episodes - episode_reward: 0.793 [-0.030, 2.166] - loss: 0.001 - mean_absolute_error: 0.018 - mean_q: 0.882

Interval 46 (450000 steps performed)
10000/10000 [==============================] - 1262s - reward: 0.0093
63 episodes - episode_reward: 1.449 [-0.430, 2.264] - loss: 0.001 - mean_absolute_error: 0.019 - mean_q: 0.877

Interval 47 (460000 steps performed)
10000/10000 [==============================] - 1731s - reward: 0.0090
59 episodes - episode_reward: 1.550 [0.783, 2.474] - loss: 0.001 - mean_absolute_error: 0.020 - mean_q: 0.872

Interval 48 (470000 steps performed)
10000/10000 [==============================] - 1225s - reward: 0.0074
48 episodes - episode_reward: 1.531 [-0.072, 2.470] - loss: 0.001 - mean_absolute_error: 0.020 - mean_q: 0.867

Interval 49 (480000 steps performed)
10000/10000 [==============================] - 1362s - reward: 0.0090
57 episodes - episode_reward: 1.589 [0.436, 2.325] - loss: 0.001 - mean_absolute_error: 0.019 - mean_q: 0.870

Interval 50 (490000 steps performed)
10000/10000 [==============================] - 1736s - reward: 0.0108
done, took 70741.709 seconds
